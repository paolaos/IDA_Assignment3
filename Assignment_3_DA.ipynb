{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 0: library imports and macro variables\n",
    "\n",
    "import pandas as pd # used mainly for data handling and operations\n",
    "import matplotlib.pyplot as plt # used to plot the k accuracy results\n",
    "from sklearn import preprocessing, metrics # preprocessing tools for the normalisers\n",
    "from sklearn.impute import SimpleImputer # imputer used during preprocessing\n",
    "from sklearn.model_selection import train_test_split # split the training data into train and test sets\n",
    "from sklearn.neighbors import KNeighborsClassifier # implementation of the knn algorithm\n",
    "\n",
    "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', \n",
    "                       'loan', 'contact', 'month', 'day_of_week', 'poutcome'] # used to segregate the columns \n",
    "# that needed numerical conversion\n",
    "\n",
    "SOLUTION_ROWS = 8238 # macro variable to ennumerate the final csv correctly\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1: importing and preprocessing data\n",
    "\n",
    "# 1.1 Import training and testing sets, create new csv files\n",
    "training_set_file = pd.read_csv('Assignment3_TrainingSet.csv')\n",
    "testing_set_file = pd.read_csv('Assignment3_TestingSet.csv')\n",
    "\n",
    "\n",
    "# 1.2 Delete marital rows with unknown values for the training set, for cleaner delivery\n",
    "altered_training_set_file = training_set_file[(training_set_file['marital'] != 'unknown') | \n",
    "                                              (training_set_file['job'] != 'unknown')]\n",
    "altered_testing_set_file = testing_set_file\n",
    "                                            \n",
    "\n",
    "# 1.3 Define generic functions for value imputation and normalisation\n",
    "\n",
    "def impute_values(column_name, dataset_file):\n",
    "    imputer = SimpleImputer(strategy='most_frequent', missing_values='unknown')\n",
    "    imputer = imputer.fit(dataset_file[[column_name]])\n",
    "    dataset_file[column_name] = \\\n",
    "        imputer.transform(dataset_file[[column_name]]).ravel() \n",
    "    \n",
    "\n",
    "def normalise_values(column_name, isMinMax, dataset_file):\n",
    "    scaler = preprocessing.MinMaxScaler() if isMinMax else preprocessing.StandardScaler()\n",
    "    scaler = scaler.fit(dataset_file[[column_name]])\n",
    "    dataset_file[column_name] = scaler.transform(dataset_file[[column_name]]).ravel()\n",
    "    \n",
    "    \n",
    "def categorical_to_numerical(dataset_file):\n",
    "    for column in categorical_columns:\n",
    "        dataset_file[column] = altered_training_set_file[column].astype('category')\n",
    "        \n",
    "    cat_columns = dataset_file.select_dtypes(['category']).columns\n",
    "    dataset_file[cat_columns] = dataset_file[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "\n",
    "# 1.4 Value imputation through simple imputers on education, housing and loan for the training set\n",
    "# In the case of the testing set, no tuples could be erased so 'marital' and 'job' were also imputed\n",
    "\n",
    "impute_values('education', altered_testing_set_file)\n",
    "impute_values('housing', altered_testing_set_file)\n",
    "impute_values('loan', altered_testing_set_file)\n",
    "impute_values('marital', altered_testing_set_file)\n",
    "impute_values('job', altered_testing_set_file)\n",
    "\n",
    "impute_values('education', altered_training_set_file)\n",
    "impute_values('housing', altered_training_set_file)\n",
    "impute_values('loan', altered_training_set_file)\n",
    "\n",
    "# 1.5 Normalise duration, consumer confidence index and consumer price index for both data sets\n",
    "\n",
    "normalise_values('cons.price.idx', True, altered_testing_set_file)\n",
    "normalise_values('cons.conf.idx', True, altered_testing_set_file)\n",
    "normalise_values('duration', False, altered_testing_set_file)\n",
    "\n",
    "normalise_values('cons.price.idx', True, altered_training_set_file)\n",
    "normalise_values('cons.conf.idx', True, altered_training_set_file)\n",
    "normalise_values('duration', False, altered_training_set_file)\n",
    "\n",
    "# 1.6 Convert all categorical values to numerical on both data sets\n",
    "\n",
    "categorical_to_numerical(altered_testing_set_file)\n",
    "categorical_to_numerical(altered_training_set_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2: training set partition and knn model generation\n",
    "\n",
    "\n",
    "# Part 2.1: separate Final-Y from the rest of the columns\n",
    "final_y_column = altered_training_set_file['Final_Y']\n",
    "training_set = altered_training_set_file.drop(columns='Final_Y')\n",
    "\n",
    "\n",
    "# Part 2.2: create 70/30 partition, shuffling first\n",
    "x_train, x_final_y, y_train, y_final_y = \\\n",
    "    train_test_split(training_set, final_y_column, shuffle=True, train_size=0.7)\n",
    "\n",
    "# Part 2.3: find the best k. Use knn for each possible k, write accuracy results compared to testing data and compare\n",
    "k_range = range(1, 150)\n",
    "scores = {}\n",
    "scores_list = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_prediction = knn.predict(x_final_y)\n",
    "    scores[k] = metrics.accuracy_score(y_final_y, y_prediction)\n",
    "    scores_list.append(scores[k])\n",
    "\n",
    "\n",
    "# Part 2.4: k evaluation, plotting relationship between k and testing accuracy\n",
    "plt.plot(k_range, scores_list)\n",
    "plt.xlabel('Value of k for KNN')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Part 2.5: storing ideal k as a variable for part 3\n",
    "ideal_k = max(scores_list)\n",
    "ideal_k_index = scores_list.index(ideal_k) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3: ideal k model implemenation and prediction\n",
    "\n",
    "# Part 3.1: re-create the model using the ideal k and the entire training set\n",
    "knn = KNeighborsClassifier(n_neighbors=ideal_k_index)\n",
    "knn.fit(training_set, final_y_column)\n",
    "\n",
    "# Part 3.2: create an ascending numeric list for the final predicted results\n",
    "prediction_row_id = list(range(SOLUTION_ROWS+1))\n",
    "prediction_row_id.pop(0)\n",
    "\n",
    "# Part 3.3: use the testing csv file to predict the values with the trained model, \n",
    "# and put the results in a new csv file\n",
    "testing_set = altered_testing_set_file\n",
    "predicted_testing_set = pd.DataFrame()\n",
    "predicted_testing_set['row ID'] = prediction_row_id\n",
    "predicted_testing_set['Final_Y'] = knn.predict(testing_set)\n",
    "predicted_testing_set.to_csv('Assignment3_Result.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
